{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***This notebook is a part of chapter 4.3***"
      ],
      "metadata": {
        "id": "WiO6P8zdHQwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Linear regression"
      ],
      "metadata": {
        "id": "K0UGKg6QFtng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression # import linear regression model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Create an example dataset for linear regression\n",
        "# Generate random data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(40, 1)\n",
        "y = 4 + 3 * X + np.random.randn(40, 1)\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "data = pd.DataFrame(data=np.hstack((X, y)), columns=['X', 'y'])\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(\"Example Dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Sample data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ftDzITiupj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare code for linear regression\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y) # find weight (a,b) of input\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate the performance metrics\n",
        "mse = mean_squared_error(y, y_pred) # lower is good\n",
        "r2 = r2_score(y, y_pred) # close to 1 is good\n",
        "\n",
        "print(\"\\nLinear Regression Model Performance:\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label='Predicted line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression Fit')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cZIAt0zPFM_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Polynomial regression"
      ],
      "metadata": {
        "id": "_6UWJv3dZFhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Create an example dataset for polynomial regression"
      ],
      "metadata": {
        "id": "LimADh-GZgTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate random data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 6 * np.random.rand(40, 1) - 3\n",
        "y = 0.5 * X**4 - 3* X**2 + X + 2 + np.random.randn(40, 1) # degree 4 polinomial\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "data = pd.DataFrame(data=np.hstack((X, y)), columns=['X', 'y'])\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(\"Example Dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Scatter plot of data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AA7rXgpjZifF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Lower degree polynomial\n",
        "Example of Lower degree polynomial.\n"
      ],
      "metadata": {
        "id": "gBgwPtVgZIAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform polynomial feature transformation\n",
        "poly = PolynomialFeatures(degree=5, include_bias=False)\n",
        "X_poly = poly.fit_transform(X) # it will include 4 column to dataset (x^2 to x^5)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model on the polynomial features\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "X_all = np.arange(-3,3,0.01).reshape(-1,1)\n",
        "X_all_poly = poly.transform(X_all)\n",
        "y_all_pred = model.predict(X_all_poly)\n",
        "\n",
        "# Calculate the performance metrics\n",
        "mse_poly = mean_squared_error(y, y_pred)\n",
        "r2_poly = r2_score(y, y_pred)\n",
        "\n",
        "print(\"\\nPolynomial Regression Model Performance (degree=2):\")\n",
        "print(f\"Mean Squared Error: {mse_poly}\")\n",
        "print(f\"R^2 Score: {r2_poly}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.scatter(X, y_pred, color='red', label='Predicted data')\n",
        "plt.plot(X_all, y_all_pred, color='green',label = 'fitted curve')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression Fit (degree=3)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cql1gnT9H38T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Higher degree polynomial\n",
        "Example of higher degree polynomial that results in overfitting."
      ],
      "metadata": {
        "id": "ZFNj0_vDZSut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform polynomial feature transformation with higher degree\n",
        "poly_high = PolynomialFeatures(degree=15, include_bias=False)\n",
        "X_poly_high = poly_high.fit_transform(X)\n",
        "\n",
        "# Create a new Linear Regression model\n",
        "model_high = LinearRegression()\n",
        "\n",
        "# Train the model on the higher degree polynomial features\n",
        "model_high.fit(X_poly_high, y)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_poly_high_pred = model_high.predict(X_poly_high)\n",
        "\n",
        "X_all = np.arange(-3,3,0.01).reshape(-1,1)\n",
        "X_all_poly_high = poly_high.fit_transform(X_all)\n",
        "y_all_pred_high = model_high.predict(X_all_poly_high)\n",
        "\n",
        "# Calculate the performance metrics\n",
        "mse_poly_high = mean_squared_error(y, y_poly_high_pred)\n",
        "r2_poly_high = r2_score(y, y_poly_high_pred)\n",
        "\n",
        "print(\"\\nPolynomial Regression Model Performance (degree=10):\")\n",
        "print(f\"Mean Squared Error: {mse_poly_high}\")\n",
        "print(f\"R^2 Score: {r2_poly_high}\")\n",
        "\n",
        "# Plot the results for the higher degree polynomial\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.scatter(X, y_poly_high_pred, color='red', label='Predicted data')\n",
        "plt.plot(X_all, y_all_pred_high, color='green',label = 'fitted curve')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression Fit (degree=15) - Overfitting Example')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "djGmOz1aX_zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Ridge and lasso regression"
      ],
      "metadata": {
        "id": "lFFdFyKycpHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Example code for ridge and lasso regression"
      ],
      "metadata": {
        "id": "jA4elllEd7wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Create an example dataset for Ridge and Lasso regression\n",
        "# Generate random data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Add some noise and higher-order terms to make the data more complex\n",
        "X_poly = np.hstack((X, X**2, X**3))\n",
        "y += 0.5 * np.random.randn(100, 1)\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "data = pd.DataFrame(data=np.hstack((X_poly, y)), columns=['X1', 'X2', 'X3', 'y'])\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(\"Example Dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 2: Train and evaluate Ridge and Lasso regression models\n",
        "# Create Ridge and Lasso Regression models\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "\n",
        "# Train the models\n",
        "ridge_model.fit(X, y)\n",
        "lasso_model.fit(X, y)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "ridge_pred = ridge_model.predict(X)\n",
        "lasso_pred = lasso_model.predict(X)\n",
        "\n",
        "# Calculate the performance metrics for Ridge\n",
        "mse_ridge = mean_squared_error(y, ridge_pred)\n",
        "r2_ridge = r2_score(y, ridge_pred)\n",
        "\n",
        "print(\"\\nRidge Regression Model Performance:\")\n",
        "print(f\"Mean Squared Error: {mse_ridge}\")\n",
        "print(f\"R^2 Score: {r2_ridge}\")\n",
        "\n",
        "# Calculate the performance metrics for Lasso\n",
        "mse_lasso = mean_squared_error(y, lasso_pred)\n",
        "r2_lasso = r2_score(y, lasso_pred)\n",
        "\n",
        "print(\"\\nLasso Regression Model Performance:\")\n",
        "print(f\"Mean Squared Error: {mse_lasso}\")\n",
        "print(f\"R^2 Score: {r2_lasso}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Ridge plot\n",
        "plt.scatter(X[:, 0], y, color='blue', label='Actual data')\n",
        "plt.plot(X[:, 0], ridge_pred, color='red', label='Ridge predicted data')\n",
        "plt.plot(X[:, 0], lasso_pred, color='green', label='Lasso predicted data')\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Ridge Regression Fit')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BvR6VL8xdLN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Difference between Ridge and Lasso Regression\n",
        "####Ridge Regression:\n",
        "\n",
        "- Ridge regression, also known as Tikhonov regularization, adds a penalty term to the linear regression cost function. This penalty term is the L2 norm (sum of the squares of the coefficients).\n",
        "- The penalty term in Ridge regression helps to shrink the coefficients of less important features, but it does not drive them to zero. This means that all features are retained in the model, even if they contribute minimally.\n",
        "- Ridge regression is useful when you have many small/medium-sized features that collectively contribute to the target variable.\n",
        "\n",
        "####Lasso Regression:\n",
        "\n",
        "- Lasso regression (Least Absolute Shrinkage and Selection Operator) adds a penalty term to the linear regression cost function. This penalty term is the L1 norm (sum of the absolute values of the coefficients).\n",
        "- The penalty term in Lasso regression can drive some coefficients to zero, effectively performing feature selection. This means that Lasso regression can eliminate some features entirely if they are not important.\n",
        "- Lasso regression is useful when you want to create a simpler model that includes only the most significant features."
      ],
      "metadata": {
        "id": "1SRH4D_jdY-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "# Generate random data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 6 * np.random.rand(40, 1) - 3\n",
        "y = 0.5 * X**4 - 3* X**2 + X + 2 + np.random.randn(40, 1)\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "data = pd.DataFrame(data=np.hstack((X, y)), columns=['X', 'y'])\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(\"Example Dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Scatter plot of data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XdOTXypSvOWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform polynomial feature transformation with higher degree\n",
        "poly_high = PolynomialFeatures(degree=15, include_bias=False)\n",
        "X_poly_high = poly_high.fit_transform(X)\n",
        "\n",
        "# Create a new regression model using ridge and lasso\n",
        "# Try to uncomment the following code.\n",
        "# model_high = LinearRegression()\n",
        "# model_high = Ridge(alpha=1)\n",
        "model_high = Lasso(alpha=0.1)\n",
        "\n",
        "# Train the model on the higher degree polynomial features\n",
        "model_high.fit(X_poly_high, y)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_poly_high_pred = model_high.predict(X_poly_high)\n",
        "\n",
        "X_all = np.arange(-3,3,0.01).reshape(-1,1)\n",
        "X_all_poly_high = poly_high.fit_transform(X_all)\n",
        "y_all_pred_high = model_high.predict(X_all_poly_high)\n",
        "\n",
        "# Calculate the performance metrics\n",
        "mse_poly_high = mean_squared_error(y, y_poly_high_pred)\n",
        "r2_poly_high = r2_score(y, y_poly_high_pred)\n",
        "\n",
        "print(\"\\nPolynomial Regression Model Performance (degree=10):\")\n",
        "print(f\"Mean Squared Error: {mse_poly_high}\")\n",
        "print(f\"R^2 Score: {r2_poly_high}\")\n",
        "\n",
        "# Plot the results for the higher degree polynomial\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.scatter(X, y_poly_high_pred, color='red', label='Predicted data')\n",
        "plt.plot(X_all, y_all_pred_high, color='green',label = 'fitted curve')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression Fit (degree=15) - Overfitting Example')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zwrpu_AHvKSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}